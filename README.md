# VAR: Making GPT-style autoregressive model surpass diffusion for the first timeðŸš€

This is the official implementation of [Visual Autoregressive Modeling: Scalable Image Generation via Next-Scale Prediction](https://arxiv.org/abs/2404.02905) using pytorch.

![abs](https://github.com/FoundationVision/VAR/assets/39692511/9850df90-20b1-4f29-8592-e3526d16d755)

## What's new here?

### ðŸ”¥ A new visual generative model is proposed: VAR

Visual AutoRegressive modeling (VAR) is a new visual generation paradigm that redefines the autoregressive learning on images as coarse-to-fine "next-scale prediction" or "next-resolution prediction", diverging from the standard raster-scan "next-token prediction".

<p align="center">
<img src="https://github.com/FoundationVision/VAR/assets/39692511/3e12655c-37dc-4528-b923-ec6c4cfef178" width=72%>
<p>

### ðŸ”¥ For the first time, GPT-style autoregressive models beat diffusion in image synthesis
<p align="center">
<img src="https://github.com/FoundationVision/VAR/assets/39692511/cc30b043-fa4e-4d01-a9b1-e50650d5675d" width=72%>
<p>


### ðŸ”¥ Power-law Scaling Laws observed in VAR transformers:


<p align="center">
<img src="https://github.com/FoundationVision/VAR/assets/39692511/c35fb56e-896e-4e4b-9fb9-7a1c38513804" width=72%>
<p>
<p align="center">
<img src="https://github.com/FoundationVision/VAR/assets/39692511/91d7b92c-8fc3-44d9-8fb4-73d6cdb8ec1e" width=72%>
<p>


### ðŸ”¥ Zero-shot generalizability:

<p align="center">
<img src="https://github.com/FoundationVision/VAR/assets/39692511/a54a4e52-6793-4130-bae2-9e459a08e96a" width=72%>
<p>

#### See our [paper](https://arxiv.org/abs/2404.02905) for more analysis, discussions, and evaluations.

