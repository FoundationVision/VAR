# VAR: Making GPT-style autoregressive model surpass diffusion for the first timeğŸš€

This is the official implementation of [Visual Autoregressive Modeling: Scalable Image Generation via Next-Scale Prediction](https://arxiv.org/abs/2404.02905) using pytorch.

### **NOTE: Mark your calendars! ğŸ“… Our code goes live and ready for action before 9:00 AM, UTC on 4/4/2024. Feel free to hit that star â­ or keep eyes peeled with a watch ğŸ‘“ for the latest updateğŸ¤—!**

![abs](https://github.com/FoundationVision/VAR/assets/39692511/9850df90-20b1-4f29-8592-e3526d16d755)

<br>

## What's new here?

### ğŸ”¥ A new visual generative model is proposed: VAR

Visual AutoRegressive modeling (VAR) is a new visual generation paradigm that redefines the autoregressive learning on images as coarse-to-fine "next-scale prediction" or "next-resolution prediction", diverging from the standard raster-scan "next-token prediction".

<p align="center">
<img src="https://github.com/FoundationVision/VAR/assets/39692511/3e12655c-37dc-4528-b923-ec6c4cfef178" width=72%>
<p>

### ğŸ”¥ For the first time, GPT-style autoregressive models beat diffusion in image synthesis
<p align="center">
<img src="https://github.com/FoundationVision/VAR/assets/39692511/cc30b043-fa4e-4d01-a9b1-e50650d5675d" width=62%>
<p>


### ğŸ”¥ Power-law Scaling Laws observed in VAR transformers:


<p align="center">
<img src="https://github.com/FoundationVision/VAR/assets/39692511/c35fb56e-896e-4e4b-9fb9-7a1c38513804" width=72%>
<p>
<p align="center">
<img src="https://github.com/FoundationVision/VAR/assets/39692511/91d7b92c-8fc3-44d9-8fb4-73d6cdb8ec1e" width=72%>
<p>


### ğŸ”¥ Zero-shot generalizability:

<p align="center">
<img src="https://github.com/FoundationVision/VAR/assets/39692511/a54a4e52-6793-4130-bae2-9e459a08e96a" width=72%>
<p>

#### See our [paper](https://arxiv.org/abs/2404.02905) for more analysis, discussions, and evaluations.

