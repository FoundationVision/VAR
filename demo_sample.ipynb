{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### ðŸš€ For an interactive experience, head over to our [demo platform](https://var.vision/demo) and dive right in! ðŸŒŸ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false,
    "is_executing": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/BS/var-training/work/var/venv-var/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[constructor]  ==== flash_if_available=True (0/16), fused_if_available=True (fusing_add_ln=0/16, fusing_mlp=0/16) ==== \n",
      "    [VAR config ] embed_dim=1024, num_heads=16, depth=16, mlp_ratio=4.0\n",
      "    [drop ratios ] drop_rate=0.0, attn_drop_rate=0.0, drop_path_rate=0.0666667 (tensor([0.0000, 0.0044, 0.0089, 0.0133, 0.0178, 0.0222, 0.0267, 0.0311, 0.0356,\n",
      "        0.0400, 0.0444, 0.0489, 0.0533, 0.0578, 0.0622, 0.0667]))\n",
      "\n",
      "[init_weights] VAR with init_std=0.0180422\n",
      "prepare finished.\n"
     ]
    }
   ],
   "source": [
    "################## 1. Download checkpoints and build models\n",
    "import os\n",
    "import os.path as osp\n",
    "import torch, torchvision\n",
    "import random\n",
    "import numpy as np\n",
    "import PIL.Image as PImage, PIL.ImageDraw as PImageDraw\n",
    "setattr(torch.nn.Linear, 'reset_parameters', lambda self: None)     # disable default parameter init for faster speed\n",
    "setattr(torch.nn.LayerNorm, 'reset_parameters', lambda self: None)  # disable default parameter init for faster speed\n",
    "from models import VQVAE, build_vae_var\n",
    "\n",
    "MODEL_DEPTH = 16    # TODO: =====> please specify MODEL_DEPTH <=====\n",
    "assert MODEL_DEPTH in {16, 20, 24, 30}\n",
    "\n",
    "\n",
    "# download checkpoint\n",
    "hf_home = 'https://huggingface.co/FoundationVision/var/resolve/main'\n",
    "vae_ckpt, var_ckpt = 'vae_ch160v4096z32.pth', f'var_d{MODEL_DEPTH}.pth'\n",
    "if not osp.exists(vae_ckpt): os.system(f'wget {hf_home}/{vae_ckpt}')\n",
    "if not osp.exists(var_ckpt): os.system(f'wget {hf_home}/{var_ckpt}')\n",
    "\n",
    "# build vae, var\n",
    "patch_nums = (1, 2, 3, 4, 5, 6, 8, 10, 13, 16)\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "if 'vae' not in globals() or 'var' not in globals():\n",
    "    vae, var = build_vae_var(\n",
    "        V=4096, Cvae=32, ch=160, share_quant_resi=4,    # hard-coded VQVAE hyperparameters\n",
    "        device=device, patch_nums=patch_nums,\n",
    "        num_classes=1000, depth=MODEL_DEPTH, shared_aln=False,\n",
    "    )\n",
    "\n",
    "# load checkpoints\n",
    "vae.load_state_dict(torch.load(vae_ckpt, map_location='cpu'), strict=True)\n",
    "var.load_state_dict(torch.load(var_ckpt, map_location='cpu'), strict=True)\n",
    "vae.eval(), var.eval()\n",
    "for p in vae.parameters(): p.requires_grad_(False)\n",
    "for p in var.parameters(): p.requires_grad_(False)\n",
    "print(f'prepare finished.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "level 0 x: x.shape=torch.Size([16, 1, 1024])\n",
      "\n",
      "level 1 x: x.shape=torch.Size([16, 4, 1024])\n",
      "\n",
      "level 2 x: x.shape=torch.Size([16, 9, 1024])\n",
      "\n",
      "level 3 x: x.shape=torch.Size([16, 16, 1024])\n",
      "\n",
      "level 4 x: x.shape=torch.Size([16, 25, 1024])\n",
      "\n",
      "level 5 x: x.shape=torch.Size([16, 36, 1024])\n",
      "\n",
      "level 6 x: x.shape=torch.Size([16, 64, 1024])\n",
      "\n",
      "level 7 x: x.shape=torch.Size([16, 100, 1024])\n",
      "\n",
      "level 8 x: x.shape=torch.Size([16, 169, 1024])\n",
      "\n",
      "level 9 x: x.shape=torch.Size([16, 256, 1024])\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/BS/var-training/work/var/venv-var/lib/python3.11/site-packages/torch/nn/modules/conv.py:456: UserWarning: Plan failed with a cudnnException: CUDNN_BACKEND_EXECUTION_PLAN_DESCRIPTOR: cudnnFinalize Descriptor Failed cudnn_status: CUDNN_STATUS_NOT_SUPPORTED (Triggered internally at ../aten/src/ATen/native/cudnn/Conv_v8.cpp:919.)\n",
      "  return F.conv2d(input, weight, bias, self.stride,\n"
     ]
    }
   ],
   "source": [
    "############################# 2. Sample with classifier-free guidance\n",
    "\n",
    "# set args\n",
    "seed = 2137 #@param {type:\"number\"}\n",
    "torch.manual_seed(seed)\n",
    "num_sampling_steps = 250 #@param {type:\"slider\", min:0, max:1000, step:1} # IRRELEVANT -- variable not used\n",
    "cfg = 4 #@param {type:\"slider\", min:1, max:10, step:0.1}\n",
    "class_labels = (980, 980, 437, 437, 22, 22, 562, 562)  #@param {type:\"raw\"}\n",
    "more_smooth = False # True for more smooth output\n",
    "\n",
    "# seed\n",
    "torch.manual_seed(seed)\n",
    "random.seed(seed)\n",
    "np.random.seed(seed)\n",
    "torch.backends.cudnn.deterministic = True\n",
    "torch.backends.cudnn.benchmark = False\n",
    "\n",
    "# run faster\n",
    "tf32 = True\n",
    "torch.backends.cudnn.allow_tf32 = bool(tf32)\n",
    "torch.backends.cuda.matmul.allow_tf32 = bool(tf32)\n",
    "torch.set_float32_matmul_precision('high' if tf32 else 'highest')\n",
    "\n",
    "# sample\n",
    "B = len(class_labels)\n",
    "label_B: torch.LongTensor = torch.tensor(class_labels, device=device)\n",
    "with torch.inference_mode():\n",
    "    with torch.autocast('cuda', enabled=True, dtype=torch.float16, cache_enabled=True):    # using bfloat16 can be faster\n",
    "        recon_B3HW = var.autoregressive_infer_cfg(B=B, label_B=label_B, cfg=cfg, top_k=900, top_p=0.95, g_seed=seed, more_smooth=more_smooth)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "kf.service.services: KApplicationTrader: mimeType \"x-scheme-handler/file\" not found\n",
      "kf.i18n.kuit: \"Unknown subcue ':whatsthis,' in UI marker in context {@info:whatsthis, %1 the action's text}.\"\n",
      "org.kde.kdegraphics.gwenview.lib: Unresolved raw mime type  \"image/x-samsung-srw\"\n"
     ]
    }
   ],
   "source": [
    "\n",
    "chw = torchvision.utils.make_grid(recon_B3HW, nrow=8, padding=0, pad_value=1.0)\n",
    "chw = chw.permute(1, 2, 0).mul_(255).cpu().numpy()\n",
    "chw = PImage.fromarray(chw.astype(np.uint8))\n",
    "chw.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "kf.service.services: KApplicationTrader: mimeType \"x-scheme-handler/file\" not found\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "kf.i18n.kuit: \"Unknown subcue ':whatsthis,' in UI marker in context {@info:whatsthis, %1 the action's text}.\"\n",
      "org.kde.kdegraphics.gwenview.lib: Unresolved raw mime type  \"image/x-samsung-srw\"\n"
     ]
    }
   ],
   "source": [
    "# chw = torchvision.utils.make_grid(recon_B3HW, nrow=8, padding=0, pad_value=1.0)\n",
    "# chw = chw.permute(1, 2, 0).mul_(255).cpu().numpy()\n",
    "# chw = PImage.fromarray(chw.astype(np.uint8))\n",
    "# chw.show()\n",
    "# recon_B3HW."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = recon_B3HW.type(torch.float32).to(device)#.add_(-0.5).mul_(2)\n",
    "f = vae.quant_conv(vae.encoder(x))\n",
    "# image_pyramid = vae.img_to_idxBl(optimized_image, patch_nums)\n",
    "\n",
    "# image_pyramid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reconstructed = vae.idxBl_to_img(image_pyramid, same_shape=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reconstructed[6].shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Encoding the input "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([8, 3, 256, 256])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "recon_B3HW.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'cuda'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "kf.service.services: KApplicationTrader: mimeType \"x-scheme-handler/file\" not found\n",
      "kf.i18n.kuit: \"Unknown subcue ':whatsthis,' in UI marker in context {@info:whatsthis, %1 the action's text}.\"\n",
      "org.kde.kdegraphics.gwenview.lib: Unresolved raw mime type  \"image/x-samsung-srw\"\n"
     ]
    }
   ],
   "source": [
    "# x = recon_B3HW\n",
    "# x = torch.rand(8, 3, 256, 256, device=device)\n",
    "\n",
    "x = recon_B3HW.type(torch.float32).to(device)#.add_(-0.5).mul_(2)\n",
    "f = vae.quant_conv(vae.encoder(x))\n",
    "\n",
    "# image = vae.fhat_to_img(f)\n",
    "# f = vae.quant_conv(vae.encoder(image))\n",
    "\n",
    "# image = vae.fhat_to_img(f)\n",
    "# f = vae.quant_conv(vae.encoder(image))\n",
    "\n",
    "# image = vae.fhat_to_img(f)\n",
    "# f = vae.quant_conv(vae.encoder(image))\n",
    "\n",
    "image = vae.fhat_to_img(f).clamp_(0, 1)\n",
    "\n",
    "# x = recon_B3HW.type(torch.float32).to(device)#.add_(-0.5).mul_(2)\n",
    "# f = vae.quant_conv(vae.encoder(x))\n",
    "# image = vae.fhat_to_img(f).add_(1).mul_(0.5)\n",
    "\n",
    "chw = torchvision.utils.make_grid(image, nrow=8, padding=0, pad_value=1.0)\n",
    "chw = chw.permute(1, 2, 0).mul_(255).cpu().numpy()\n",
    "chw = PImage.fromarray(chw.astype(np.uint8))\n",
    "chw.show()\n",
    "\n",
    "# ls_f_hat_BChw = vae.quantize.f_to_idxBl_or_fhat(f, to_fhat=False, v_patch_nums=patch_nums)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "kf.service.services: KApplicationTrader: mimeType \"x-scheme-handler/file\" not found\n",
      "kf.i18n.kuit: \"Unknown subcue ':whatsthis,' in UI marker in context {@info:whatsthis, %1 the action's text}.\"\n",
      "org.kde.kdegraphics.gwenview.lib: Unresolved raw mime type  \"image/x-samsung-srw\"\n"
     ]
    }
   ],
   "source": [
    "x = recon_B3HW.type(torch.float32).to(device)#.add_(-0.5).mul_(2)\n",
    "f = vae.quant_conv(vae.encoder(x))\n",
    "f_hat, _, _ = vae.quantize(f)\n",
    "\n",
    "\n",
    "image = vae.fhat_to_img(f_hat)\n",
    "\n",
    "# x = recon_B3HW.type(torch.float32).to(device)#.add_(-0.5).mul_(2)\n",
    "# f = vae.quant_conv(vae.encoder(x))\n",
    "# image = vae.fhat_to_img(f).add_(1).mul_(0.5)\n",
    "\n",
    "chw = torchvision.utils.make_grid(image, nrow=8, padding=0, pad_value=1.0)\n",
    "chw = chw.permute(1, 2, 0).mul_(255).cpu().numpy()\n",
    "chw = PImage.fromarray(chw.astype(np.uint8))\n",
    "chw.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# vae.quantize.forward(f)\n",
    "# assert False    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'f' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[5], line 6\u001b[0m\n\u001b[1;32m      3\u001b[0m patch_nums \u001b[38;5;241m=\u001b[39m (\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m2\u001b[39m, \u001b[38;5;241m3\u001b[39m, \u001b[38;5;241m4\u001b[39m, \u001b[38;5;241m5\u001b[39m, \u001b[38;5;241m6\u001b[39m, \u001b[38;5;241m8\u001b[39m, \u001b[38;5;241m10\u001b[39m, \u001b[38;5;241m13\u001b[39m, \u001b[38;5;241m16\u001b[39m)\n\u001b[1;32m      4\u001b[0m last_patch_id \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m\n\u001b[0;32m----> 6\u001b[0m quant_pyramid, f_hat \u001b[38;5;241m=\u001b[39m vae\u001b[38;5;241m.\u001b[39mquantize\u001b[38;5;241m.\u001b[39mf_to_quant_pyramid_and_f_hat(\u001b[43mf\u001b[49m, patch_nums, last_patch_id)\n\u001b[1;32m      8\u001b[0m residual \u001b[38;5;241m=\u001b[39m f \u001b[38;5;241m-\u001b[39m f_hat\n",
      "\u001b[0;31mNameError\u001b[0m: name 'f' is not defined"
     ]
    }
   ],
   "source": [
    "from torch.nn import functional as F\n",
    "\n",
    "patch_nums = (1, 2, 3, 4, 5, 6, 8, 10, 13, 16)\n",
    "last_patch_id = -1\n",
    "\n",
    "quant_pyramid, f_hat = vae.quantize.f_to_quant_pyramid_and_f_hat(f, patch_nums, last_patch_id)\n",
    "\n",
    "residual = f - f_hat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "len(quant_pyramid)=0)\n"
     ]
    }
   ],
   "source": [
    "print(f\"{len(quant_pyramid)=})\")\n",
    "if quant_pyramid:\n",
    "    print(f\"{quant_pyramid[0].shape=}\")\n",
    "# assert False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using the pyramid as the input for transformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "current_level=-1, next_level=0\n",
      "next_token_map lvl current_level=-1 next_level=0: next_token_map.shape=torch.Size([16, 1, 1024])\n",
      "\n"
     ]
    }
   ],
   "source": [
    "predicted_residual = var.autoregressive_single_step_prediction(\n",
    "    quant_pyramid=quant_pyramid, \n",
    "    f_hat=f_hat, \n",
    "    label_B=label_B,) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Single step prediction "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "next_level=0, patch_nums[next_level]=1\n",
      "next_level=1, patch_nums[next_level]=2\n",
      "next_level=2, patch_nums[next_level]=3\n",
      "next_level=3, patch_nums[next_level]=4\n",
      "next_level=4, patch_nums[next_level]=5\n",
      "next_level=5, patch_nums[next_level]=6\n",
      "next_level=6, patch_nums[next_level]=8\n",
      "next_level=7, patch_nums[next_level]=10\n",
      "next_level=8, patch_nums[next_level]=13\n",
      "next_level=9, patch_nums[next_level]=16\n"
     ]
    }
   ],
   "source": [
    "f_hat = torch.zeros_like(f)\n",
    "\n",
    "for next_level in range(len(patch_nums)):\n",
    "    current_level = next_level - 1\n",
    "\n",
    "    print(f\"{next_level=}, patch_nums[next_level]={patch_nums[next_level]}\")\n",
    "\n",
    "    predicted_residual = var.autoregressive_single_step_prediction(\n",
    "        current_level=current_level, \n",
    "        f_hat=f_hat, \n",
    "        label_B=label_B,\n",
    "    ) \n",
    "\n",
    "    f_hat = f_hat + predicted_residual\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "kf.service.services: KApplicationTrader: mimeType \"x-scheme-handler/file\" not found\n",
      "kf.i18n.kuit: \"Unknown subcue ':whatsthis,' in UI marker in context {@info:whatsthis, %1 the action's text}.\"\n",
      "org.kde.kdegraphics.gwenview.lib: Unresolved raw mime type  \"image/x-samsung-srw\"\n"
     ]
    }
   ],
   "source": [
    "image = vae.fhat_to_img(f_hat.detach()).clamp_(0, 1)\n",
    "\n",
    "chw = torchvision.utils.make_grid(image, nrow=8, padding=0, pad_value=1.0)\n",
    "chw = chw.permute(1, 2, 0).mul_(255).cpu().numpy()\n",
    "chw = PImage.fromarray(chw.astype(np.uint8))\n",
    "chw.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([8, 32, 16, 16])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([8, 32, 16, 16])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predicted_residual.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Optimisation POC in latent space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "current_level=-1, next_level=0\n",
      "next_token_map lvl current_level=-1 next_level=0: next_token_map.shape=torch.Size([4, 1, 1024])\n",
      "\n",
      "coarsness_step: -1, loss: 0.39253973960876465\n",
      "current_level=0, next_level=1\n",
      "next_token_map lvl current_level=0 next_level=1: next_token_map.shape=torch.Size([4, 4, 1024])\n",
      "\n",
      "coarsness_step: 0, loss: 0.0009515469428151846\n",
      "current_level=1, next_level=2\n",
      "next_token_map lvl current_level=1 next_level=2: next_token_map.shape=torch.Size([4, 9, 1024])\n",
      "\n",
      "coarsness_step: 1, loss: 0.002446998842060566\n",
      "current_level=2, next_level=3\n",
      "next_token_map lvl current_level=2 next_level=3: next_token_map.shape=torch.Size([4, 16, 1024])\n",
      "\n",
      "coarsness_step: 2, loss: 0.004130854737013578\n",
      "current_level=3, next_level=4\n",
      "next_token_map lvl current_level=3 next_level=4: next_token_map.shape=torch.Size([4, 25, 1024])\n",
      "\n",
      "coarsness_step: 3, loss: 0.006683871150016785\n",
      "current_level=4, next_level=5\n",
      "next_token_map lvl current_level=4 next_level=5: next_token_map.shape=torch.Size([4, 36, 1024])\n",
      "\n",
      "coarsness_step: 4, loss: 0.009225272573530674\n",
      "current_level=5, next_level=6\n",
      "next_token_map lvl current_level=5 next_level=6: next_token_map.shape=torch.Size([4, 64, 1024])\n",
      "\n",
      "coarsness_step: 5, loss: 0.01421268004924059\n",
      "current_level=6, next_level=7\n",
      "next_token_map lvl current_level=6 next_level=7: next_token_map.shape=torch.Size([4, 100, 1024])\n",
      "\n",
      "coarsness_step: 6, loss: 0.02462838962674141\n",
      "current_level=7, next_level=8\n",
      "next_token_map lvl current_level=7 next_level=8: next_token_map.shape=torch.Size([4, 169, 1024])\n",
      "\n",
      "coarsness_step: 7, loss: 0.04235750064253807\n",
      "current_level=8, next_level=9\n",
      "next_token_map lvl current_level=8 next_level=9: next_token_map.shape=torch.Size([4, 256, 1024])\n",
      "\n",
      "coarsness_step: 8, loss: 0.10407813638448715\n"
     ]
    }
   ],
   "source": [
    "\n",
    "f = torch.rand([2, 32, 16, 16], requires_grad=True, device=device)\n",
    "\n",
    "class_labels = (22, 562)  #@param {type:\"raw\"}\n",
    "label_B: torch.LongTensor = torch.tensor(class_labels, device=device)\n",
    "\n",
    "optimizer = torch.optim.Adam([f], lr=0.1)\n",
    "\n",
    "steps_per_coarsness = 1\n",
    "\n",
    "for coarsness_step in range(-1, len((patch_nums)) - 1):\n",
    "    for i in range(steps_per_coarsness):\n",
    "        optimizer.zero_grad()\n",
    "        loss = get_optimisation_loss(f, coarsness_step)\n",
    "        patch_weight = patch_nums[coarsness_step]**2 / patch_nums[len(patch_nums)-1]**2\n",
    "        step_weight = (steps_per_coarsness - i) /steps_per_coarsness\n",
    "        \n",
    "        loss*= patch_weight * step_weight\n",
    "\n",
    "        if i % 500 == 0:\n",
    "            print(f'coarsness_step: {coarsness_step}, loss: {loss.item()}')\n",
    "\n",
    "        \n",
    "        loss.backward()\n",
    "        optimizer.step()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "AssertionError",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[19], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m\n",
      "\u001b[0;31mAssertionError\u001b[0m: "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "kf.i18n.kuit: \"Unknown subcue ':whatsthis,' in UI marker in context {@info:whatsthis, %1 the action's text}.\"\n",
      "org.kde.kdegraphics.gwenview.lib: Unresolved raw mime type  \"image/x-samsung-srw\"\n",
      "kf.service.services: KApplicationTrader: mimeType \"x-scheme-handler/file\" not found\n",
      "kf.i18n.kuit: \"Unknown subcue ':whatsthis,' in UI marker in context {@info:whatsthis, %1 the action's text}.\"\n",
      "org.kde.kdegraphics.gwenview.lib: Unresolved raw mime type  \"image/x-samsung-srw\"\n"
     ]
    }
   ],
   "source": [
    "assert False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 2, 3, 4, 5, 6, 8, 10, 13, 16)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "var.patch_nums"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "image = vae.fhat_to_img(f).add_(1).mul_(0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "kf.service.services: KApplicationTrader: mimeType \"x-scheme-handler/file\" not found\n",
      "kf.i18n.kuit: \"Unknown subcue ':whatsthis,' in UI marker in context {@info:whatsthis, %1 the action's text}.\"\n",
      "org.kde.kdegraphics.gwenview.lib: Unresolved raw mime type  \"image/x-samsung-srw\"\n"
     ]
    }
   ],
   "source": [
    "chw = torchvision.utils.make_grid(image, nrow=8, padding=0, pad_value=1.0)\n",
    "chw = chw.permute(1, 2, 0).mul_(255).cpu().numpy()\n",
    "chw = PImage.fromarray(chw.astype(np.uint8))\n",
    "chw.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Random coarsness step\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_optimisation_loss(f, last_patch_id, f_hat=None, quant_pyramid=None):\n",
    "    if f_hat is None or quant_pyramid is None:\n",
    "        quant_pyramid, f_hat = vae.quantize.f_to_quant_pyramid_and_f_hat(f, patch_nums, last_patch_id)\n",
    "\n",
    "    f_residual = f - f_hat\n",
    "\n",
    "#     predicted_residual = var.autoregressive_single_step_prediction(\n",
    "#         quant_pyramid=quant_pyramid, \n",
    "#         f_hat=f_hat, \n",
    "#         label_B=label_B, \n",
    "#         cfg=cfg, \n",
    "#         top_k=900, \n",
    "#         top_p=0.95)\n",
    "    \n",
    "#     # TODO add stability loss \n",
    "#     # Ensure f_residual scaled down to the \"current\" patch size should be close to zero (?)\n",
    "\n",
    "#     return F.l1_loss(predicted_residual, f_residual)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.nn import functional as F\n",
    "\n",
    "\n",
    "def get_downsampling_regularisation_loss(f_residual, patch_size):\n",
    "    downsampled = F.interpolate(f_residual, size=(patch_size, patch_size), mode='area')\n",
    "\n",
    "    return F.mse_loss(downsampled, torch.zeros_like(downsampled)) # L1 regularisation\n",
    "\n",
    "\n",
    "def get_next_step_loss(quant_pyramid, f_residual, f_hat, label_B, patch_size:int=None):\n",
    "    predicted_residual = var.autoregressive_single_step_prediction(\n",
    "        quant_pyramid=quant_pyramid, \n",
    "        f_hat=f_hat, \n",
    "        label_B=label_B, \n",
    "        cfg=cfg, \n",
    "        top_k=900, \n",
    "        top_p=0.95)\n",
    "    \n",
    "    if patch_size is not None:\n",
    "        f_residual = F.interpolate(f_residual, size=(patch_size, patch_size), mode='area')\n",
    "        predicted_residual = F.interpolate(predicted_residual, size=(patch_size, patch_size), mode='area')\n",
    "    \n",
    "    return F.mse_loss(predicted_residual, f_residual)\n",
    "\n",
    "def get_optimisation_loss(f, last_patch_id, f_hat=None, quant_pyramid=None, regularisation_weight=0, downsample_next_step_loss=True):\n",
    "    if f_hat is None or quant_pyramid is None:\n",
    "        quant_pyramid, f_hat = vae.quantize.f_to_quant_pyramid_and_f_hat(f, patch_nums, last_patch_id)\n",
    "\n",
    "    f_residual = f - f_hat\n",
    "\n",
    "    if downsample_next_step_loss:\n",
    "        predicted_patch_size = patch_nums[last_patch_id + 1] # next patch size\n",
    "        next_step_loss = get_next_step_loss(quant_pyramid, f_residual, f_hat, label_B, patch_size=predicted_patch_size)\n",
    "    else:\n",
    "        next_step_loss = get_next_step_loss(quant_pyramid, f_residual, f_hat, label_B)\n",
    "\n",
    "    total_loss = next_step_loss\n",
    "\n",
    "    if regularisation_weight > 0:\n",
    "        regularisation_losses = [\n",
    "            get_downsampling_regularisation_loss(f_residual, patch_size)\n",
    "            for patch_size in patch_nums[:last_patch_id]\n",
    "        ]\n",
    "\n",
    "        total_loss += sum(regularisation_losses) * regularisation_weight\n",
    "\n",
    "        \n",
    "    return total_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_14492/934706667.py:7: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  f = torch.tensor(vae.quant_conv(vae.encoder(img).detach()), requires_grad=True, device=device)\n",
      "  1%|          | 5/500 [00:00<00:11, 44.48it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss: 0.19761879742145538\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 11%|â–ˆ         | 56/500 [00:01<00:14, 30.62it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss: 0.04332656413316727\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 21%|â–ˆâ–ˆ        | 106/500 [00:03<00:13, 30.07it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss: 0.011186541058123112\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 31%|â–ˆâ–ˆâ–ˆâ–      | 157/500 [00:05<00:11, 29.29it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss: 0.0033746412955224514\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 41%|â–ˆâ–ˆâ–ˆâ–ˆ      | 206/500 [00:06<00:09, 30.08it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss: 0.0011509687174111605\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 51%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 257/500 [00:08<00:08, 29.24it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss: 0.0004355781420599669\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 61%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 306/500 [00:10<00:06, 29.84it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss: 0.00017993705114349723\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 71%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 357/500 [00:12<00:04, 29.00it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss: 7.930067658890039e-05\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 406/500 [00:13<00:03, 29.70it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss: 3.549300163285807e-05\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 91%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 456/500 [00:15<00:01, 29.73it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss: 1.3702514479518868e-05\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 500/500 [00:17<00:00, 29.17it/s]\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "# f = torch.randn([4, 32, 16, 16], requires_grad=True, device=device) \n",
    "# class_labels = (980, 437, 22, 562)  #@param {type:\"raw\"}\n",
    "\n",
    "\n",
    "img = recon_B3HW.float().clone().clamp_(0, 1).to(device)\n",
    "f = torch.tensor(vae.quant_conv(vae.encoder(img).detach()), requires_grad=True, device=device)\n",
    "class_labels = (980, 980, 437, 437, 22, 22, 562, 562)  #@param {type:\"raw\"}\n",
    "\n",
    "label_B: torch.LongTensor = torch.tensor(class_labels, device=device)\n",
    "\n",
    "optimizer = torch.optim.Adam([f], lr=0.01)\n",
    "total_steps = 500\n",
    "\n",
    "min_coarsness = len(patch_nums) - 2 #-1 #len(patch_nums) - 4\n",
    "max_coarsness = len(patch_nums) - 2 \n",
    "\n",
    "\n",
    "\n",
    "quant_pyramid, f_hat = vae.quantize.f_to_quant_pyramid_and_f_hat(f, patch_nums, max_coarsness)\n",
    "\n",
    "assert len(quant_pyramid) == len(patch_nums) - 1\n",
    "# quant_pyramid, f_hat = None, None\n",
    "\n",
    "for i in tqdm(range(total_steps), total=total_steps):\n",
    "    coarsness_step = np.random.randint(min_coarsness, max_coarsness + 1)\n",
    "\n",
    "    loss = get_optimisation_loss(f, coarsness_step, f_hat, quant_pyramid, regularisation_weight=0.0)\n",
    "\n",
    "    patch_grad_scale = patch_nums[coarsness_step]**2 / patch_nums[len(patch_nums)-1]**2\n",
    "    loss*= patch_grad_scale\n",
    "    \n",
    "    step_weight = (total_steps - i) / total_steps\n",
    "    loss*= step_weight\n",
    "\n",
    "    level_ratio = (coarsness_step + 2)/ (len(patch_nums) + 1) \n",
    "    level_weight = np.sqrt(level_ratio)\n",
    "    loss*= level_weight\n",
    "\n",
    "    if i % 50 ==0:\n",
    "        print(f'loss: {loss.item()}')\n",
    "    # print(f'{level_ratio=}, {level_weight=}, {step_weight=}, {patch_grad_scale=}, {loss.item()=}')\n",
    "\n",
    "    loss.backward()\n",
    "\n",
    "    optimizer.step()\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "f = f.detach()\n",
    "\n",
    "    # if i % 10 == 0:\n",
    "    #     print(f'coarsness_step: {coarsness_step}, loss: {loss.item()}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "kf.service.services: KApplicationTrader: mimeType \"x-scheme-handler/file\" not found\n",
      "kf.i18n.kuit: \"Unknown subcue ':whatsthis,' in UI marker in context {@info:whatsthis, %1 the action's text}.\"\n",
      "org.kde.kdegraphics.gwenview.lib: Unresolved raw mime type  \"image/x-samsung-srw\"\n"
     ]
    }
   ],
   "source": [
    "image = vae.fhat_to_img(f.detach()).clamp_(0, 1)\n",
    "chw = torchvision.utils.make_grid(image, nrow=8, padding=0, pad_value=1.0)\n",
    "chw = chw.permute(1, 2, 0).mul_(255).cpu().numpy()\n",
    "chw = PImage.fromarray(chw.astype(np.uint8))\n",
    "chw.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_11540/2004129305.py:1: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  original_f = torch.tensor(vae.quant_conv(vae.encoder(img).detach()), requires_grad=False, device=device)\n",
      "kf.service.services: KApplicationTrader: mimeType \"x-scheme-handler/file\" not found\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "kf.i18n.kuit: \"Unknown subcue ':whatsthis,' in UI marker in context {@info:whatsthis, %1 the action's text}.\"\n",
      "org.kde.kdegraphics.gwenview.lib: Unresolved raw mime type  \"image/x-samsung-srw\"\n",
      "kf.service.services: KApplicationTrader: mimeType \"x-scheme-handler/file\" not found\n",
      "kf.i18n.kuit: \"Unknown subcue ':whatsthis,' in UI marker in context {@info:whatsthis, %1 the action's text}.\"\n",
      "org.kde.kdegraphics.gwenview.lib: Unresolved raw mime type  \"image/x-samsung-srw\"\n"
     ]
    }
   ],
   "source": [
    "original_f = torch.tensor(vae.quant_conv(vae.encoder(img).detach()), requires_grad=False, device=device)\n",
    "image = vae.fhat_to_img(original_f.detach()).clamp_(0, 1)\n",
    "chw = torchvision.utils.make_grid(image, nrow=8, padding=0, pad_value=1.0)\n",
    "chw = chw.permute(1, 2, 0).mul_(255).cpu().numpy()\n",
    "chw = PImage.fromarray(chw.astype(np.uint8))\n",
    "chw.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(patch_nums)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "patch_sizes =  (1, 2, 3, 4, 5, 6, 8, 10, 13, 16)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "kf.kio.widgets.kdirmodel: No node found for item that was just removed: QUrl(\"file:///tmp/.org.chromium.Chromium.xf4D1D\")\n"
     ]
    }
   ],
   "source": [
    "print(\"patch_sizes = \", patch_nums)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fixed F-hat incremetal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "coarsness_level: -1, loss: 0.0006090999025363968\n",
      "coarsness_level: 0, loss: 0.00017674557398780655\n",
      "coarsness_level: 1, loss: 7.816978732463561e-05\n",
      "coarsness_level: 2, loss: 0.000136359025294917\n",
      "coarsness_level: 3, loss: 0.00022669045838961596\n",
      "coarsness_level: 4, loss: 0.00021566336376152353\n",
      "coarsness_level: 5, loss: 0.0003899050076426731\n",
      "coarsness_level: 6, loss: 0.0004408489482989985\n",
      "coarsness_level: 7, loss: 0.0005911831812500716\n",
      "coarsness_level: 8, loss: 0.001166960721561788\n"
     ]
    }
   ],
   "source": [
    "\n",
    "f = torch.zeros([2, 32, 16, 16], requires_grad=True, device=device)\n",
    "\n",
    "class_labels = (22, 562)  #@param {type:\"raw\"}\n",
    "label_B: torch.LongTensor = torch.tensor(class_labels, device=device)\n",
    "\n",
    "optimizer = torch.optim.Adam([f], lr=0.05)\n",
    "\n",
    "steps_per_coarsness = 500\n",
    "\n",
    "for coarsness_level in range(-1, len((patch_nums)) - 1):\n",
    "    losses = []\n",
    "\n",
    "    quant_pyramid, f_hat = vae.quantize.f_to_quant_pyramid_and_f_hat(f, patch_nums, coarsness_level)\n",
    "\n",
    "    f_clone = f.clone().detach()\n",
    "    \n",
    "    for i in range(steps_per_coarsness):\n",
    "        optimizer.zero_grad()\n",
    "        loss = get_optimisation_loss(f, coarsness_level, f_clone, quant_pyramid, regularisation_weight=0)\n",
    "        #patch_weight = 1 #patch_nums[coarsness_level]**2 / patch_nums[len(patch_nums)-1]**2\n",
    "        #step_weight = (steps_per_coarsness - i) /steps_per_coarsness\n",
    "        \n",
    "        #loss*= patch_weight * step_weight\n",
    "        losses.append(loss.item())\n",
    "\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    print(f'coarsness_level: {coarsness_level}, loss: {np.mean(losses)}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "kf.service.services: KApplicationTrader: mimeType \"x-scheme-handler/file\" not found\n",
      "kf.i18n.kuit: \"Unknown subcue ':whatsthis,' in UI marker in context {@info:whatsthis, %1 the action's text}.\"\n",
      "org.kde.kdegraphics.gwenview.lib: Unresolved raw mime type  \"image/x-samsung-srw\"\n"
     ]
    }
   ],
   "source": [
    "image = vae.fhat_to_img(f.detach()).clamp_(0, 1)\n",
    "chw = torchvision.utils.make_grid(image, nrow=8, padding=0, pad_value=1.0)\n",
    "chw = chw.permute(1, 2, 0).mul_(255).cpu().numpy()\n",
    "chw = PImage.fromarray(chw.astype(np.uint8))\n",
    "chw.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# All at once optimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'torch' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtqdm\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m tqdm\n\u001b[0;32m----> 3\u001b[0m f \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241m.\u001b[39mzeros([\u001b[38;5;241m2\u001b[39m, \u001b[38;5;241m32\u001b[39m, \u001b[38;5;241m16\u001b[39m, \u001b[38;5;241m16\u001b[39m], requires_grad\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, device\u001b[38;5;241m=\u001b[39mdevice)\n\u001b[1;32m      5\u001b[0m class_labels \u001b[38;5;241m=\u001b[39m (\u001b[38;5;241m980\u001b[39m, \u001b[38;5;241m437\u001b[39m)  \u001b[38;5;66;03m#@param {type:\"raw\"}\u001b[39;00m\n\u001b[1;32m      6\u001b[0m label_B: torch\u001b[38;5;241m.\u001b[39mLongTensor \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mtensor(class_labels, device\u001b[38;5;241m=\u001b[39mdevice)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'torch' is not defined"
     ]
    }
   ],
   "source": [
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "f = torch.zeros([2, 32, 16, 16], requires_grad=True, device=device)\n",
    "\n",
    "class_labels = (980, 437)  #@param {type:\"raw\"}\n",
    "label_B: torch.LongTensor = torch.tensor(class_labels, device=device)\n",
    "\n",
    "optimizer = torch.optim.Adam([f], lr=0.05)\n",
    "\n",
    "total_steps = 2000\n",
    "\n",
    "for i in tqdm(range(total_steps), total=total_steps):\n",
    "    loss = 0\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "    final_patch = int(i/total_steps * len(patch_nums)) \n",
    "\n",
    "    for coarsness_level in range(-1, final_patch):\n",
    "        # patch_weight = patch_nums[coarsness_level+1]**2 / patch_nums[len(patch_nums)-1]**2\n",
    "        loss += get_optimisation_loss(f, coarsness_level, regularisation_weight=0) #* patch_weight\n",
    "        \n",
    "\n",
    "    if i % 100 == 0:\n",
    "        print(f'loss: {loss.item()}')\n",
    "\n",
    "    #step_weight = (total_steps - i) / total_steps\n",
    "\n",
    "\n",
    "\n",
    "    #loss*= step_weight\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "\n",
    "# for coarsness_step in range(-1, len((patch_nums)) - 1):\n",
    "#     for i in range(steps_per_coarsness):\n",
    "#         optimizer.zero_grad()\n",
    "#         loss = get_optimisation_loss(f, coarsness_step)\n",
    "#         patch_weight = patch_nums[coarsness_step]**2 / patch_nums[len(patch_nums)-1]**2\n",
    "#         step_weight = (steps_per_coarsness - i) /steps_per_coarsness\n",
    "        \n",
    "#         loss*= patch_weight * step_weight\n",
    "\n",
    "#         if i % 500 == 0:\n",
    "#             print(f'coarsness_step: {coarsness_step}, loss: {loss.item()}')\n",
    "\n",
    "        \n",
    "#         loss.backward()\n",
    "#         optimizer.step()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "kf.service.services: KApplicationTrader: mimeType \"x-scheme-handler/file\" not found\n",
      "kf.i18n.kuit: \"Unknown subcue ':whatsthis,' in UI marker in context {@info:whatsthis, %1 the action's text}.\"\n",
      "org.kde.kdegraphics.gwenview.lib: Unresolved raw mime type  \"image/x-samsung-srw\"\n"
     ]
    }
   ],
   "source": [
    "image = vae.fhat_to_img(f.detach()).clamp_(0, 1)\n",
    "chw = torchvision.utils.make_grid(image, nrow=8, padding=0, pad_value=1.0)\n",
    "chw = chw.permute(1, 2, 0).mul_(255).cpu().numpy()\n",
    "chw = PImage.fromarray(chw.astype(np.uint8))\n",
    "chw.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# VAR prediction\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for coarsness_level in range(-1, len((patch_nums)) - 1):\n",
    "    losses = []\n",
    "\n",
    "    quant_pyramid, f_hat = vae.quantize.f_to_quant_pyramid_and_f_hat(f, patch_nums, coarsness_level)\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv-var",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  },
  "polyglot_notebook": {
   "kernelInfo": {
    "defaultKernelName": "csharp",
    "items": [
     {
      "aliases": [],
      "name": "csharp"
     }
    ]
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
